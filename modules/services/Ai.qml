pragma Singleton
import QtQuick
import Quickshell
import Quickshell.Io
import qs.config
import qs.modules.services
import "ai"
import "ai/strategies"

Singleton {
    id: root

    // ============================================ 
    // PROPERTIES
    // ============================================ 

    property string dataDir: (Quickshell.env("XDG_DATA_HOME") || (Quickshell.env("HOME") + "/.local/share")) + "/Ambxst"
    property string chatDir: dataDir + "/chats"
    property string tmpDir: "/tmp/ambxst-ai"

    property list<AiModel> models: []

    property AiModel currentModel: models.length > 0 ? models[0] : null
    property bool persistenceReady: false

    onCurrentModelChanged: {
        if (persistenceReady && currentModel) {
            StateService.set("lastAiModel", currentModel.model)
        }
    }
    


    function restoreModel() {
        const lastModelId = StateService.get("lastAiModel", "gemini-pro");
        for (let i = 0; i < models.length; i++) {
            if (models[i].model === lastModelId) {
                currentModel = models[i];
                break;
            }
        }
        persistenceReady = true;
    }

    Connections {
        target: StateService
        function onStateLoaded() {
            restoreModel();
        }
    }

    Component.onCompleted: {
        // Try restoration immediately if possible, or wait for signal
        if (StateService.initialized) {
            restoreModel();
        }
        
        // Dynamic fetch if no models
        if (models.length === 0) {
            fetchAvailableModels();
        }
        
        // Initialize chat
        reloadHistory();
        createNewChat();
    }

    property ApiStrategy currentStrategy: !currentModel ? geminiStrategy : 
                                          (currentModel.api_format === "openai" ? openaiStrategy : 
                                          (currentModel.api_format === "mistral" ? mistralStrategy : geminiStrategy))

    // Strategies
    property GeminiApiStrategy geminiStrategy: GeminiApiStrategy {}
    property OpenAiApiStrategy openaiStrategy: OpenAiApiStrategy {}
    property MistralApiStrategy mistralStrategy: MistralApiStrategy {}

    // State
    property bool isLoading: false
    property string lastError: ""
    property string responseBuffer: ""

    // Current Chat
    property var currentChat: [] // Array of { role: "user"|"assistant", content: "..." }
    property string currentChatId: ""
    
    // Chat History List (files)
    // Chat History List (files)
    property var chatHistory: [] 

    // ============================================ 
    // TOOLS
    // ============================================
    function regenerateResponse(index) {
        if (index < 0 || index >= currentChat.length) return;
        
        // Remove this message and everything after it
        let newChat = currentChat.slice(0, index);
        currentChat = newChat;
        
        isLoading = true;
        lastError = "";
        
        makeRequest();
    }

    function updateMessage(index, newContent) {
        if (index < 0 || index >= currentChat.length) return;
        
        let newChat = Array.from(currentChat);
        let msg = newChat[index];
        msg.content = newContent;
        newChat[index] = msg;
        
        currentChat = newChat;
        saveCurrentChat();
    }

    property var systemTools: [
        {
            name: "run_shell_command",
            description: "Execute a shell command on the user's system (Linux/Hyprland). Use this to list files, control the system, or run utilities. Output will be returned.",
            parameters: {
                type: "OBJECT",
                properties: {
                    command: {
                        type: "STRING",
                        description: "The shell command to run (e.g. 'ls -la', 'ip addr', 'hyprctl clients')"
                    }
                },
                required: ["command"]
            }
        }
    ]

    // ============================================ 
    // INIT
    // ============================================ 
    function deleteChat(id) {
        if (id === currentChatId) {
            createNewChat();
        }
        
        let filename = chatDir + "/" + id + ".json";
        deleteChatProcess.command = ["rm", filename];
        deleteChatProcess.running = true;
    }



    // ============================================ 
    // LOGIC
    // ============================================ 

    function setModel(modelName) {
        for (let i = 0; i < models.length; i++) {
            if (models[i].name === modelName) {
                currentModel = models[i];
                updateStrategy();
                return;
            }
        }
    }

    function updateStrategy() {
        if (!currentModel) return;
        switch (currentModel.api_format) {
            case "gemini": currentStrategy = geminiStrategy; break;
            case "openai": currentStrategy = openaiStrategy; break;
            case "mistral": currentStrategy = mistralStrategy; break;
            default: currentStrategy = geminiStrategy;
        }
    }

    function getApiKey(model) {
        if (!model.requires_key) return "";
        return Quickshell.env(model.key_id) || "";
    }

    function processCommand(text) {
        let cmd = text.trim();
        if (!cmd.startsWith("/")) return false;
        
        let parts = cmd.split(" ");
        let command = parts[0].toLowerCase();
        let args = parts.slice(1).join(" ");
        
        switch (command) {
            case "/clear":
                createNewChat();
                return true;

            case "/model":
                if (args) {
                    // Fuzzy search or exact match
                    let found = false;
                    for (let i = 0; i < models.length; i++) {
                         if (models[i].name.toLowerCase().includes(args.toLowerCase()) || 
                             models[i].model.toLowerCase() === args.toLowerCase()) {
                             setModel(models[i].name);
                             found = true;
                             break;
                         }
                    }
                    if (!found) {
                        pushSystemMessage("Model '" + args + "' not found.");
                    } else {
                        pushSystemMessage("Switched to model: " + currentModel.name);
                    }
                } else {
                    // Request UI to show selection popup
                    modelSelectionRequested();
                }
                return true;
            case "/help":
                pushSystemMessage(
                    "ðŸ¤– **Assistant Commands**\n\n" +
                    "**`/clear`**\n" +
                    "Resets the current session and starts a fresh conversation context.\n\n" +
                    "**`/model [name]`**\n" +
                    "Switches the active AI model.\n" +
                    "â€¢ **List models:** Type `/model` without arguments.\n" +
                    "â€¢ **Switch:** Type `/model gemini` or `/model mistral`.\n\n" +
                    "**`/help`**\n" +
                    "Shows this help message.\n\n" +
                    "ðŸ’¡ **Tips:**\n" +
                    "â€¢ **Edit:** Click the pen icon on any message to modify it.\n" +
                    "â€¢ **Regenerate:** Click the refresh icon to get a new response.\n" +
                    "â€¢ **Copy:** Use the copy button to grab code or text."
                );
                return true;
        }
        
        return false;
    }

    function pushSystemMessage(text) {
        let newChat = Array.from(currentChat);
        newChat.push({ role: "system", content: text });
        currentChat = newChat;
    }

    // Function Call Handling
    function approveCommand(index) {
        let msg = currentChat[index];
        if (!msg.functionCall) return;
        
        // Update message state
        let newChat = Array.from(currentChat);
        newChat[index].functionPending = false;
        newChat[index].functionApproved = true;
        currentChat = newChat;
        saveCurrentChat();
        
        // Execute
        let args = msg.functionCall.args;
        if (msg.functionCall.name === "run_shell_command") {
            commandExecutionProc.command = ["bash", "-c", args.command];
            commandExecutionProc.targetIndex = index;
            commandExecutionProc.running = true;
        }
    }
    
    function rejectCommand(index) {
        let newChat = Array.from(currentChat);
        newChat[index].functionPending = false;
        newChat[index].functionApproved = false;
        
        // Add system message indicating rejection
        newChat.push({
            role: "function",
            name: newChat[index].functionCall.name,
            content: "User rejected the command execution."
        });
        
        currentChat = newChat;
        saveCurrentChat();
        
        // Continue conversation
        makeRequest();
    }

    function sendMessage(text) {
        if (text.trim() === "") return;
        
        if (processCommand(text)) return;

        isLoading = true;
        lastError = "";
        
        // Add user message to UI immediately
        let userMsg = { role: "user", content: text };
        let newChat = Array.from(currentChat);
        newChat.push(userMsg);
        currentChat = newChat;
        
        makeRequest();
    }
    
    function makeRequest() {
        // Prepare Request
        let apiKey = getApiKey(currentModel);
        if (!apiKey && currentModel.requires_key) {
            lastError = "API Key missing for " + currentModel.name;
            isLoading = false;
            
            let errChat = Array.from(currentChat);
            errChat.push({ role: "assistant", content: "Error: " + lastError });
            currentChat = errChat;
            return;
        }

        let endpoint = currentStrategy.getEndpoint(currentModel, apiKey);
        let headers = currentStrategy.getHeaders(apiKey);
        
        // Include system prompt
        let messages = [];
        if (Config.ai.systemPrompt) {
            messages.push({ role: "system", content: Config.ai.systemPrompt });
        }
        // Add history (simple version: all messages)
        // Note: Gemini doesn't support 'system' role in messages list the same way, handled in strategy
        for (let i = 0; i < currentChat.length; i++) {
            messages.push(currentChat[i]);
        }
        
        // Pass tools
        let body = currentStrategy.getBody(messages, currentModel, systemTools);
        
        // Write body to temp file
        writeTempBody(JSON.stringify(body), headers, endpoint);
    }

    function writeTempBody(jsonBody, headers, endpoint) {
        // Create tmp dir
        requestProcess.command = ["mkdir", "-p", tmpDir];
        requestProcess.step = "mkdir";
        requestProcess.payload = { body: jsonBody, headers: headers, endpoint: endpoint };
        requestProcess.running = true;
    }

    function executeRequest(payload) {
        let bodyPath = tmpDir + "/body.json";
        
        // Write body.json
        // We use a separate process call for writing to avoid command line length limits
        writeBodyProcess.command = ["sh", "-c", "echo '" + payload.body.replace(/'/g, "'\\''") + "' > " + bodyPath];
        writeBodyProcess.payload = payload; // pass through
        writeBodyProcess.running = true;
    }
    
    function runCurl(payload) {
        let bodyPath = tmpDir + "/body.json";
        let headerArgs = payload.headers.map(h => "-H \"" + h + "\"").join(" ");
        
        let curlCmd = "curl -s -X POST \"" + payload.endpoint + "\" " + headerArgs + " -d @" + bodyPath;
        
        curlProcess.command = ["bash", "-c", curlCmd];
        curlProcess.running = true;
    }

    // ============================================ 
    // PROCESSES
    // ============================================ 

    Process {
        id: requestProcess
        property string step: ""
        property var payload: ({})
        
        onExited: exitCode => {
            if (exitCode === 0 && step === "mkdir") {
                executeRequest(payload);
            } else {
                root.lastError = "Failed to create temp directory (mkdir exited with " + exitCode + ")";
                root.isLoading = false;
                let errChat = Array.from(root.currentChat);
                errChat.push({ role: "assistant", content: "Error: " + root.lastError });
                root.currentChat = errChat;
            }
        }
    }

    Process {
        id: writeBodyProcess
        property var payload: ({})
        stderr: StdioCollector { id: writeBodyStderr }
        
        onExited: exitCode => {
            if (exitCode === 0) {
                runCurl(payload);
            } else {
                root.lastError = "Failed to write request body: " + writeBodyStderr.text;
                root.isLoading = false;
                let errChat = Array.from(root.currentChat);
                errChat.push({ role: "assistant", content: "Error: " + root.lastError });
                root.currentChat = errChat;
            }
        }
    }

    Process {
        id: curlProcess
        
        stdout: StdioCollector { id: curlStdout }
        stderr: StdioCollector { id: curlStderr }
        
        onExited: exitCode => {
            root.isLoading = false;
            if (exitCode === 0) {
                let responseText = curlStdout.text;
                let reply = root.currentStrategy.parseResponse(responseText);
                
                let newChat = Array.from(root.currentChat);
                
                if (reply.content) {
                    newChat.push({ role: "assistant", content: reply.content });
                }
                
                if (reply.functionCall) {
                    // It's a tool call
                    let funcMsg = {
                        role: "assistant",
                        content: "I want to run a command: `" + reply.functionCall.name + "`",
                        functionCall: reply.functionCall,
                        functionPending: true, // UI will show Approve/Reject
                        geminiParts: reply.geminiParts // Store raw parts (thoughts) for API history
                    };
                    newChat.push(funcMsg);
                }
                
                root.currentChat = newChat;
                root.saveCurrentChat();
                
                // If it was just a text reply, stop loading. If it's a function, we wait for user.
                if (!reply.functionCall) {
                    root.isLoading = false;
                }
            } else {
                root.isLoading = false;
                root.lastError = "Network Request Failed: " + curlStderr.text;
                
                let errChat = Array.from(root.currentChat);
                errChat.push({ role: "assistant", content: "Error: " + root.lastError });
                root.currentChat = errChat;
            }
        }
    }
    
    Process {
        id: commandExecutionProc
        property int targetIndex: -1
        
        stdout: StdioCollector { id: cmdStdout }
        stderr: StdioCollector { id: cmdStderr }
        
        onExited: exitCode => {
             let output = cmdStdout.text + "\n" + cmdStderr.text;
             if (output.trim() === "") output = "Command executed successfully (no output).";
             
             // Add function response
             let msg = currentChat[targetIndex];
             let newChat = Array.from(currentChat);
             
             newChat.push({
                 role: "function",
                 name: msg.functionCall.name,
                 content: output
             });
             
             root.currentChat = newChat;
             root.saveCurrentChat();
             
             // Continue conversation
             root.makeRequest();
        }
    }

    // ============================================ 
    // CHAT STORAGE
    // ============================================ 
    
    function createNewChat() {
        currentChat = [];
        currentChatId = Date.now().toString();
        chatModelChanged();
    }
    
    function saveCurrentChat() {
        if (currentChat.length === 0) return;
        
        let filename = chatDir + "/" + currentChatId + ".json";
        let data = JSON.stringify(currentChat, null, 2);
        
        saveChatProcess.command = ["sh", "-c", "mkdir -p " + chatDir + " && echo '" + data.replace(/'/g, "'\\''") + "' > " + filename];
        saveChatProcess.running = true;
    }
    
    function reloadHistory() {
        // List files in chatDir
        listHistoryProcess.command = ["sh", "-c", "mkdir -p " + chatDir + " && ls -t " + chatDir + "/*.json"];
        listHistoryProcess.running = true;
    }

    function loadChat(id) {
        let filename = chatDir + "/" + id + ".json";
        loadChatProcess.targetId = id;
        loadChatProcess.command = ["cat", filename];
        loadChatProcess.running = true;
    }

    Process {
        id: saveChatProcess
        onExited: reloadHistory()
    }

    Process {
        id: deleteChatProcess
        onExited: reloadHistory()
    }
    
    Process {
        id: listHistoryProcess
        stdout: StdioCollector { id: listHistoryStdout }
        onExited: exitCode => {
            if (exitCode === 0) {
                let lines = listHistoryStdout.text.trim().split("\n");
                let history = [];
                for (let i = 0; i < lines.length; i++) {
                    let path = lines[i];
                    if (path === "") continue;
                    let filename = path.split("/").pop();
                    let id = filename.replace(".json", "");
                    history.push({ id: id, path: path });
                }
                root.chatHistory = history;
                root.historyModelChanged();
            }
        }
    }
    
    Process {
        id: loadChatProcess
        property string targetId: ""
        stdout: StdioCollector { id: loadChatStdout }
        onExited: exitCode => {
            if (exitCode === 0) {
                try {
                    root.currentChat = JSON.parse(loadChatStdout.text);
                    root.currentChatId = targetId;
                    root.chatModelChanged();
                } catch(e) {
                    console.log("Error loading chat: " + e);
                }
            }
        }
    }
    
    // ============================================ 
    // DYNAMIC MODEL FETCHING
    // ============================================ 
    
    property bool fetchingModels: false
    property int pendingFetches: 0
    
    function fetchAvailableModels() {
        if (fetchingModels) return;
        
        fetchingModels = true;
        pendingFetches = 0;
        
        // Gemini
        if (geminiStrategy && Quickshell.env("GEMINI_API_KEY")) {
            pendingFetches++;
            fetchProcessGemini.command = ["bash", "-c", "curl -s 'https://generativelanguage.googleapis.com/v1beta/models?key=" + Quickshell.env("GEMINI_API_KEY") + "'"];
            fetchProcessGemini.running = true;
        }
        
        // OpenAI
        if (openaiStrategy && Quickshell.env("OPENAI_API_KEY")) {
            pendingFetches++;
            fetchProcessOpenAi.command = ["bash", "-c", "curl -s https://api.openai.com/v1/models -H 'Authorization: Bearer " + Quickshell.env("OPENAI_API_KEY") + "'"];
            fetchProcessOpenAi.running = true;
        }
        
        // Mistral
        if (mistralStrategy && Quickshell.env("MISTRAL_API_KEY")) {
            pendingFetches++;
            fetchProcessMistral.command = ["bash", "-c", "curl -s https://api.mistral.ai/v1/models -H 'Authorization: Bearer " + Quickshell.env("MISTRAL_API_KEY") + "'"];
            fetchProcessMistral.running = true;
        }
        
        if (pendingFetches === 0) {
            fetchingModels = false;
        }
    }
    
    function checkFetchCompletion() {
        pendingFetches--;
        if (pendingFetches <= 0) {
            fetchingModels = false;
            pendingFetches = 0;
            
            // Auto-select first model if none selected
            if (!currentModel && models.length > 0) {
                 currentModel = models[0];
            }
        }
    }
    
    function mergeModels(newModels) {
        // Create a map of existing models by name to avoid duplicates
        let existingMap = {};
        for (let i = 0; i < models.length; i++) {
            existingMap[models[i].name] = true;
        }
        
        let updatedList = [];
        // Keep hardcoded/existing models first? Or allow overwriting?
        // Let's keep existing ones and append new ones.
        for (let i = 0; i < models.length; i++) {
            updatedList.push(models[i]);
        }
        
        for (let i = 0; i < newModels.length; i++) {
            let m = newModels[i];
            // Simple duplicate check by name or model ID
            let isDuplicate = false;
            for (let j=0; j<updatedList.length; j++) {
                if (updatedList[j].model === m.model) {
                    isDuplicate = true;
                    break;
                }
            }
            
            if (!isDuplicate) {
                updatedList.push(m);
            }
        }
        
        models = updatedList;
    }

    Process {
        id: fetchProcessGemini
        stdout: StdioCollector { id: fetchGeminiOut }
        onExited: exitCode => {
            if (exitCode === 0) {
                try {
                    let data = JSON.parse(fetchGeminiOut.text);
                    if (data.models) {
                        let newModels = [];
                        for (let i=0; i<data.models.length; i++) {
                            let item = data.models[i]; // name: "models/gemini-pro", displayName: "Gemini Pro"
                            let id = item.name.replace("models/", "");
                            // Filter for generative models if possible, but for now just add them
                            if (id.includes("gemini") || id.includes("flash") || id.includes("pro")) {
                                let m = aiModelFactory.createObject(root, {
                                    name: item.displayName || id,
                                    icon: "sparkles",
                                    description: item.description || "Google Gemini Model",
                                    endpoint: "https://generativelanguage.googleapis.com/v1beta/models/",
                                    model: id,
                                    api_format: "gemini",
                                    requires_key: true,
                                    key_id: "GEMINI_API_KEY"
                                });
                                if (m) newModels.push(m);
                            }
                        }
                        mergeModels(newModels);
                    }
                } catch(e) { console.log("Gemini fetch error: " + e) }
            }
            checkFetchCompletion();
        }
    }

    Process {
        id: fetchProcessOpenAi
        stdout: StdioCollector { id: fetchOpenAiOut }
        onExited: exitCode => {
            if (exitCode === 0) {
                try {
                    let data = JSON.parse(fetchOpenAiOut.text);
                    if (data.data) {
                        let newModels = [];
                        for (let i=0; i<data.data.length; i++) {
                            let item = data.data[i];
                            let id = item.id;
                            if (id.includes("gpt")) {
                                let m = aiModelFactory.createObject(root, {
                                    name: id,
                                    icon: "openai",
                                    description: "OpenAI Model",
                                    endpoint: "https://api.openai.com/v1",
                                    model: id,
                                    api_format: "openai",
                                    requires_key: true,
                                    key_id: "OPENAI_API_KEY"
                                });
                                if (m) newModels.push(m);
                            }
                        }
                        mergeModels(newModels);
                    }
                } catch(e) { console.log("OpenAI fetch error: " + e) }
            }
            checkFetchCompletion();
        }
    }

    Process {
        id: fetchProcessMistral
        stdout: StdioCollector { id: fetchMistralOut }
        onExited: exitCode => {
            if (exitCode === 0) {
                try {
                    let data = JSON.parse(fetchMistralOut.text);
                    if (data.data) {
                        let newModels = [];
                        for (let i=0; i<data.data.length; i++) {
                            let item = data.data[i];
                            let id = item.id;
                             let m = aiModelFactory.createObject(root, {
                                name: id,
                                icon: "wind",
                                description: "Mistral Model",
                                endpoint: "https://api.mistral.ai/v1",
                                model: id,
                                api_format: "mistral",
                                requires_key: true,
                                key_id: "MISTRAL_API_KEY"
                            });
                            if (m) newModels.push(m);
                        }
                        mergeModels(newModels);
                    }
                } catch(e) { console.log("Mistral fetch error: " + e) }
            }
            checkFetchCompletion();
        }
    }

    // Signals
    signal chatModelChanged()
    signal historyModelChanged()
    signal modelSelectionRequested()

    Component {
        id: aiModelFactory
        AiModel {}
    }
}

